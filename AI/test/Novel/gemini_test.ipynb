{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini 텍스트 생성\n",
    "\n",
    "* 모델 종류: https://ai.google.dev/gemini-api/docs/models/gemini?hl=ko&_gl=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 전용 입력에서 텍스트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=\"GEMINI_API_KEY\")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=[\"How does AI work?\"])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 및 이미지 입력에서 텍스트 생성\n",
    "\n",
    "* Gemini API는 텍스트와 미디어 파일을 결합한 멀티모달 입력을 지원합니다. 다음 예는 텍스트 및 이미지 입력에서 텍스트를 생성하는 방법을 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=\"GEMINI_API_KEY\")\n",
    "\n",
    "image = Image.open(\"/path/to/organ.png\")\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=[image, \"Tell me about this instrument\"])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 스트림 생성\n",
    "\n",
    "* 기본적으로 모델은 전체 텍스트 생성 프로세스를 완료한 후에 응답을 반환합니다. 전체 결과를 기다리지 않고 대신 스트리밍을 사용하여 부분 결과를 처리하면 더 빠른 상호작용을 실행할 수 있습니다.\n",
    "* 다음 예에서는 streamGenerateContent 메서드를 사용하여 스트리밍을 구현하여 텍스트 전용 입력 프롬프트에서 텍스트를 생성하는 방법을 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=\"GEMINI_API_KEY\")\n",
    "\n",
    "response = client.models.generate_content_stream(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=[\"Explain how AI works\"])\n",
    "for chunk in response:\n",
    "    print(chunk.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 채팅 대화 만들기\n",
    "\n",
    "* Gemini SDK를 사용하면 여러 번의 질문과 응답을 수집할 수 있으므로 사용자가 점진적으로 답변을 찾거나 여러 부분으로 구성된 문제와 관련하여 도움을 받을 수 있습니다. 이 SDK 기능은 대화 기록을 추적하는 인터페이스를 제공하지만, 백그라운드에서는 동일한 generateContent 메서드를 사용하여 응답을 만듭니다.\n",
    "\n",
    "* 다음 코드 예는 기본 채팅 구현을 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=\"GEMINI_API_KEY\")\n",
    "\n",
    "chat = client.chats.create(model=\"gemini-2.0-flash\")\n",
    "response = chat.send_message(\"I have 2 dogs in my house.\")\n",
    "print(response.text)\n",
    "response = chat.send_message(\"How many paws are in my house?\")\n",
    "print(response.text)\n",
    "for message in chat._curated_history:\n",
    "    print(f'role - ', message.role, end=\": \")\n",
    "    print(message.parts[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 다음 예와 같이 채팅과 함께 스트리밍을 사용할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=\"GEMINI_API_KEY\")\n",
    "\n",
    "chat = client.chats.create(model=\"gemini-2.0-flash\")\n",
    "response = chat.send_message_stream(\"I have 2 dogs in my house.\")\n",
    "for chunk in response:\n",
    "    print(chunk.text, end=\"\")\n",
    "response = chat.send_message_stream(\"How many paws are in my house?\")\n",
    "for chunk in response:\n",
    "    print(chunk.text, end=\"\")\n",
    "for message in chat._curated_history:\n",
    "    print(f'role - ', message.role, end=\": \")\n",
    "    print(message.parts[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 생성 구성\n",
    "\n",
    "* 모델에 전송하는 모든 프롬프트에는 모델이 응답을 생성하는 방식을 제어하는 매개변수가 포함됩니다. GenerationConfig를 사용하여 이러한 매개변수를 구성할 수 있습니다. 매개변수를 구성하지 않으면 모델은 기본 옵션을 사용하며 이는 모델에 따라 다를 수 있습니다.\n",
    "\n",
    "* 다음 예는 사용 가능한 여러 옵션을 구성하는 방법을 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client(api_key=\"GEMINI_API_KEY\")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=[\"Explain how AI works\"],\n",
    "    config=types.GenerateContentConfig(\n",
    "        max_output_tokens=500,\n",
    "        temperature=0.1\n",
    "    )\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시스템 안내 추가\n",
    "\n",
    "* 시스템 안내를 사용하면 특정 요구사항 및 사용 사례에 따라 모델의 동작을 조정할 수 있습니다.\n",
    "\n",
    "* 모델에 시스템 안내를 제공하면 작업을 이해하고, 보다 맞춤설정된 대답을 생성하고, 모델과의 전체 사용자 상호작용에 대한 특정 가이드라인을 준수하기 위한 추가 컨텍스트를 모델에 제공할 수 있습니다. 최종 사용자가 제공하는 프롬프트와 별도로 시스템 안내를 설정하여 제품 수준 동작을 지정할 수도 있습니다.\n",
    "\n",
    "* 모델을 초기화할 때 시스템 안내를 설정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_instruct=\"You are a cat. Your name is Neko.\"\n",
    "client = genai.Client(api_key=\"GEMINI_API_KEY\")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=sys_instruct),\n",
    "    contents=[\"your prompt here\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
